{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Author Writing Style Analysis\n",
    "\n",
    "**Authors:** Juan Arturo Abaurrea Calafell, Radu-Andrei Bourceanu\n",
    "\n",
    "---\n",
    "\n",
    "## Description\n",
    "\n",
    "This project is part of the PAN competition tasks, one of the most renowned competitions in the field of Natural Language Processing. Each year, PAN proposes different tasks in a competitive format, providing training datasets where researchers compete to propose solutions.\n",
    "\n",
    "For this assignment, we selected the **Multi-Author Writing Style Analysis 2025** task from the Authorship track.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. **Preparation**: Load and preprocess the dataset for the selected task\n",
    "2. **Training**: Design, justify, and implement a model using at least one fine-tuned Transformer with an original approach\n",
    "3. **Evaluation**: Evaluate the proposal on test/validation sets and compare results with baseline models and other published approaches\n",
    "\n",
    "---\n",
    "\n",
    "## Task Description: Multi-Author Writing Style Analysis\n",
    "\n",
    "**Dataset:** [Zenodo Record 14891299](https://zenodo.org/records/14891299)\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "The goal of the style change detection task is to identify text positions within a given multi-author document at which the author switches.\n",
    "\n",
    "### Specific Task\n",
    "\n",
    "For a given text, find all positions of writing style change at the sentence level. Specifically, for each pair of consecutive sentences, assess whether there was a style change (i.e., different authors wrote the sentences).\n",
    "\n",
    "### Dataset Difficulty Levels\n",
    "\n",
    "The dataset provides three difficulty levels with carefully controlled topic variation:\n",
    "\n",
    "| Level | Description | Topic Control |\n",
    "|-------|-------------|---------------|\n",
    "| **Easy** | Sentences cover a variety of topics | Approaches can leverage topic information |\n",
    "| **Medium** | Small topical variety | Must focus more on style |\n",
    "| **Hard** | All sentences on the same topic | Pure style-based detection required |\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "Each difficulty level is split into:\n",
    "- **Training set (70%)**: With ground truth for model development\n",
    "- **Validation set (15%)**: With ground truth for evaluation and optimization\n",
    "- **Test set (15%)**: For final evaluation\n",
    "\n",
    "All documents are in English and may contain an arbitrary number of style changes, but changes only occur between sentences (a single sentence is always authored by a single author).\n",
    "\n",
    "### Citation\n",
    "\n",
    "> Zangerle, E., Mayerl, M., Potthast, M., & Stein, B. (2025). PAN25 Multi-Author Writing Style Analysis [Data set]. Zenodo. https://doi.org/10.5281/zenodo.14891299\n",
    "\n",
    "---\n",
    "\n",
    "## Our Approach\n",
    "\n",
    "We implement and compare several transformer-based approaches:\n",
    "\n",
    "1. **Custom Lightweight Transformer**: A small transformer trained from scratch (~7M parameters)\n",
    "2. **Pretrained BERT Models**: Fine-tuned HuggingFace models (e.g., bert-mini)\n",
    "3. **Siamese Architectures**: Models that encode each sentence separately and compare embeddings\n",
    "\n",
    "To handle the significant class imbalance (most sentences do not switch author), we employ:\n",
    "- Weighted sampling during training\n",
    "- Label smoothing for regularization\n",
    "- Hyperparameter tuning to prevent overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "- **Part 1: Exploratory Data Analysis (EDA)** - Data loading, preprocessing, and analysis\n",
    "- **Part 2: Model Architecture** - Definition of encoders, classification heads, and full models\n",
    "- **Part 3: Training** - Training loop with class imbalance handling\n",
    "- **Part 4: Single Model Experiment** - Training and evaluating a single model with visualization\n",
    "- **Part 5: Model Comparison** - Training/loading multiple models and comparing test set performance\n",
    "- **Part 6: Inference** - Example predictions and interactive testing\n",
    "- **Conclusion** - Summary of findings and future improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Setup and Data Loading\n",
    "\n",
    "## 1.1 Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'  # to avoid potential issues with macOS and some Windows setups\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from dataclasses import dataclass, field\n",
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "NUM_WORKERS = 0 if os.name == 'nt' else 4  # 0 for Windows, 4 for Linux/Mac\n",
    "ALWAYS_PARSE = False  # Set to True to always re-parse the data, even if cached files exist\n",
    "ALWAYS_TRAIN = False  # Set to True to always re-train the model, even if trained models in the results directory exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Download\n",
    "\n",
    "The dataset comes from the PAN25 Multi-Author Analysis task. We will also store it locally for easy access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://zenodo.org/records/14891299/files/pan25-multi-author-analysis.zip\"\n",
    "DATA_DIR = \"data\"\n",
    "ZIP_FILE_NAME = \"pan25-multi-author-analysis.zip\"\n",
    "ZIP_FILE_PATH = os.path.join(DATA_DIR, ZIP_FILE_NAME)\n",
    "DIFFICULTY_LEVELS = ['easy', 'medium', 'hard']\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "if not os.path.exists(ZIP_FILE_PATH):\n",
    "    print(\"Downloading dataset...\")\n",
    "    response = requests.get(DATA_URL, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(ZIP_FILE_PATH, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    print(f\"Downloaded to {ZIP_FILE_PATH}\")\n",
    "else:\n",
    "    print(f\"Dataset already exists at {ZIP_FILE_PATH}\")\n",
    "\n",
    "if not all(level in os.listdir(DATA_DIR) for level in DIFFICULTY_LEVELS):\n",
    "    print(\"Extracting dataset...\")\n",
    "    with zipfile.ZipFile(ZIP_FILE_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_DIR)\n",
    "    print(f\"Extracted to {DATA_DIR}\")\n",
    "else:\n",
    "    print(\"Dataset already extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data Loading Function\n",
    "\n",
    "The `load_data` function processes the raw problem files and truth labels to create sentence pairs with binary labels:\n",
    "- **Label = False (0)**: Same author wrote both sentences\n",
    "- **Label = True (1)**: Different authors wrote the sentences\n",
    "\n",
    "The function supports:\n",
    "- Loading specific difficulty levels\n",
    "- Creating all possible sentence pairs (for data augmentation)\n",
    "- Swapping sentence order to prevent the model from relying too heavily on the position of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir: str, difficulty_levels: list[str] | str, use_all_possible_pairs: bool = False, swap_same_author_sentences: bool = False, swap_different_author_sentences: bool = False, load_problems: int = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads data from the specified directory and difficulty levels and returns it as a single dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        data_dir: Directory containing the data.\n",
    "        difficulty_levels: List of difficulty levels to load (e.g., ['easy', 'medium']).\n",
    "        use_all_possible_pairs: For sentences by the same author (consecutive 0s), creates all possible pairs.\n",
    "        load_problems: Maximum number of problem files to load per split and difficulty level. If None, loads all.\n",
    "        swap_same_author_sentences, swap_different_author_sentences: If True, creates additional rows with sentence1 and sentence2 swapped for the respective case.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns 'sentence1', 'sentence2', 'label'.\n",
    "    \"\"\"\n",
    "    if not isinstance(difficulty_levels, (list, set, tuple)):\n",
    "        difficulty_levels = [difficulty_levels]\n",
    "    \n",
    "    rows = []\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for split in os.listdir(os.path.join(data_dir, difficulty_levels[0])):\n",
    "        for level in difficulty_levels:\n",
    "            print(f\"Processing split: {split}, level: {level}\")\n",
    "            split_dir = os.path.join(data_dir, level, split)\n",
    "            \n",
    "            if not os.path.exists(split_dir):\n",
    "                continue\n",
    "            \n",
    "            problem_files = sorted([f for f in os.listdir(split_dir) if f.startswith('problem-') and f.endswith('.txt')])\n",
    "            \n",
    "            for idx, problem_file in enumerate(problem_files):\n",
    "                if load_problems is not None and idx >= load_problems:\n",
    "                    break\n",
    "                \n",
    "                problem_num = problem_file.replace('problem-', '').replace('.txt', '')\n",
    "                truth_file = f'truth-problem-{problem_num}.json'\n",
    "                \n",
    "                problem_path = os.path.join(split_dir, problem_file)\n",
    "                truth_path = os.path.join(split_dir, truth_file)\n",
    "                \n",
    "                if not os.path.exists(truth_path):\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                with open(problem_path, 'r', encoding='utf-8') as f:\n",
    "                    sentences = [line.strip() for line in f.readlines() if line.strip()]\n",
    "                \n",
    "                with open(truth_path, 'r', encoding='utf-8') as f:\n",
    "                    truth_data = json.load(f)\n",
    "                    authors = truth_data.get('authors', 0)\n",
    "                    changes = truth_data.get('changes', [])\n",
    "                \n",
    "                if len(sentences) != len(changes) + 1 or authors < 1:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "                if not use_all_possible_pairs:\n",
    "                    # Create pairs only between 2 consecutive sentences\n",
    "                    for i in range(len(sentences) - 1):\n",
    "                        rows.append({\n",
    "                            'sentence1': sentences[i],\n",
    "                            'sentence2': sentences[i + 1],\n",
    "                            'label': changes[i] != 0\n",
    "                        })\n",
    "                else:\n",
    "                    # Create all possible pairs (more complex logic for multiple authors)\n",
    "                    if authors == 2:\n",
    "                        author_sentences = [[], []]\n",
    "                        current_author = 0\n",
    "                        \n",
    "                        for i in range(len(sentences)):\n",
    "                            author_sentences[current_author].append(sentences[i])\n",
    "                            if i < len(changes) and changes[i] != 0:\n",
    "                                current_author = 1 - current_author\n",
    "                        \n",
    "                        for author_group in author_sentences:\n",
    "                            for i in range(len(author_group)):\n",
    "                                for j in range(i + 1, len(author_group)):\n",
    "                                    rows.append({\n",
    "                                        'sentence1': author_group[i],\n",
    "                                        'sentence2': author_group[j],\n",
    "                                        'label': False\n",
    "                                    })\n",
    "                        \n",
    "                        for sent_a in author_sentences[0]:\n",
    "                            for sent_b in author_sentences[1]:\n",
    "                                rows.append({\n",
    "                                    'sentence1': sent_a,\n",
    "                                    'sentence2': sent_b,\n",
    "                                    'label': True\n",
    "                                })\n",
    "                    else:\n",
    "                        i = 0\n",
    "                        while i < len(sentences):\n",
    "                            j = i\n",
    "                            while j < len(changes) and changes[j] == 0:\n",
    "                                j += 1\n",
    "                            \n",
    "                            group_size = j - i + 1\n",
    "                            \n",
    "                            if group_size > 1:\n",
    "                                for k in range(i, j + 1):\n",
    "                                    for l in range(k + 1, j + 1):\n",
    "                                        rows.append({\n",
    "                                            'sentence1': sentences[k],\n",
    "                                            'sentence2': sentences[l],\n",
    "                                            'label': False\n",
    "                                        })\n",
    "                            \n",
    "                            if j < len(changes) and changes[j] != 0:\n",
    "                                rows.append({\n",
    "                                    'sentence1': sentences[j],\n",
    "                                    'sentence2': sentences[j + 1],\n",
    "                                    'label': True\n",
    "                                })\n",
    "                            \n",
    "                            i = j + 1\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df['sentence1'] = df['sentence1'].astype('string')\n",
    "    df['sentence2'] = df['sentence2'].astype('string')\n",
    "    df['label'] = df['label'].astype('boolean')\n",
    "\n",
    "    print(f\"Total documents skipped due to mismatches: {skipped_count}\")\n",
    "    \n",
    "    if swap_same_author_sentences:\n",
    "        df_same = df[df['label'] == False].copy()\n",
    "        df_same['sentence1'], df_same['sentence2'] = df_same['sentence2'], df_same['sentence1']\n",
    "        rows_added = len(df_same)\n",
    "        df = pd.concat([df, df_same], ignore_index=True)\n",
    "        print(f\"Added {rows_added} swapped same-author sentence pairs to the dataset\")\n",
    "\n",
    "    if swap_different_author_sentences:\n",
    "        df_diff = df[df['label'] == True].copy()\n",
    "        df_diff['sentence1'], df_diff['sentence2'] = df_diff['sentence2'], df_diff['sentence1']\n",
    "        rows_added = len(df_diff)\n",
    "        df = pd.concat([df, df_diff], ignore_index=True)\n",
    "        print(f\"Added {rows_added} swapped different-author sentence pairs to the dataset\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Load and Explore Data\n",
    "\n",
    "After loading the data, it is cached to CSV to avoid re-parsing on subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LOADING_PARAMETERS = {\n",
    "    \"difficulty_levels\": DIFFICULTY_LEVELS[0],  # 'easy'\n",
    "    \"swap_different_author_sentences\": False,  # True to augment underrepresented class by swapping sentences\n",
    "    \"load_problems\": None  # Set to a number to limit data for faster experimentation\n",
    "}\n",
    "\n",
    "csv_path = os.path.join(DATA_DIR, 'loaded_data.csv')\n",
    "if not os.path.exists(csv_path) or ALWAYS_PARSE:\n",
    "    df = load_data(DATA_DIR, **DATA_LOADING_PARAMETERS)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved loaded data to {csv_path}\\n\")\n",
    "else:\n",
    "    df = pd.read_csv(csv_path, dtype={'sentence1': 'string', 'sentence2': 'string', 'label': 'boolean'})\n",
    "    print(f\"Loaded data from {csv_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Exploratory Data Analysis\n",
    "\n",
    "Let's examine the dataset characteristics including class distribution, sentence lengths, and outlier removal.\n",
    "\n",
    "### Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"First 5 rows:\")\n",
    "combined_df = pd.concat([\n",
    "    df.head(3),\n",
    "    df.sample(4, random_state=RANDOM_SEED).sort_index(),\n",
    "    df.tail(3)\n",
    "])\n",
    "display(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df['label'].value_counts()\n",
    "print(f\"\\nLabel value counts:\")\n",
    "print(label_counts)\n",
    "print(f\"\\nClass imbalance ratio: {label_counts[False] / label_counts[True]:.2f}:1 (Same:Different authors)\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "bars = ax.bar(['Same Author (False)', 'Different Authors (True)'], label_counts.values, color=colors)\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Class Distribution')\n",
    "for bar, count in zip(bars, label_counts.values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100, f'{count:,}', ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nNote: The dataset is imbalanced with {label_counts[False] / label_counts.sum():.2%} same-author pairs.\")\n",
    "print(\"We address this in different ways during training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Length Analysis\n",
    "\n",
    "We'll analyze the distribution of sentence lengths and identify outliers to clean the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df['sentence1'].str.len(), bins=50, alpha=0.7, label='Sentence 1', color='#3498db')\n",
    "axes[0].hist(df['sentence2'].str.len(), bins=50, alpha=0.7, label='Sentence 2', color='#e74c3c')\n",
    "axes[0].set_xlabel('Character Length')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Sentence Length Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].boxplot([df['sentence1'].str.len(), df['sentence2'].str.len()],\n",
    "                labels=['Sentence 1', 'Sentence 2'],\n",
    "                patch_artist=True,\n",
    "                boxprops=dict(facecolor='#3498db', alpha=0.7),\n",
    "                medianprops=dict(color='red', linewidth=2))\n",
    "axes[1].set_ylabel('Character Length')\n",
    "axes[1].set_title('Sentence Length Comparison')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = (\n",
    "    pd.concat([\n",
    "        df[['sentence1']].rename(columns={'sentence1': 'sentence'}),\n",
    "        df[['sentence2']].rename(columns={'sentence2': 'sentence'})\n",
    "    ])\n",
    "    .drop_duplicates(subset=['sentence'])\n",
    "    .assign(length=lambda x: x['sentence'].str.len())\n",
    ")\n",
    "\n",
    "def show_sentences(title, ascending):\n",
    "    print(title)\n",
    "    display(\n",
    "        all_sentences\n",
    "        .sort_values('length', ascending=ascending)\n",
    "        .head(5)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "show_sentences(\"Top 5 longest sentences:\", ascending=False)\n",
    "show_sentences(\"Top 5 shortest sentences:\", ascending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Extreme Cases\n",
    "\n",
    "We remove extreme outliers for several important reasons:\n",
    "\n",
    "1. **Data Quality Issues**: Very short sentences (< 10 characters) are often:\n",
    "   - Single punctuation marks or symbols (\"?\", \"!\", \".\")\n",
    "   - Emojis or meaningless fragments\n",
    "   - Not representative of actual writing style\n",
    "\n",
    "2. **Noise in Long Sentences**: Very long sentences (> 2000 characters) often contain:\n",
    "   - Repetitive or copy-pasted text\n",
    "   - Malformed data from scraping errors\n",
    "   - Unusual artifacts (e.g., excessive dots or special characters)\n",
    "\n",
    "3. **Model Training**: \n",
    "   - Extreme outliers can dominate loss calculations and gradient updates\n",
    "   - They don't represent typical sentence pairs in real-world authorship scenarios\n",
    "   - Can skew the distribution and make training unstable\n",
    "\n",
    "4. **Statistical Validity**:\n",
    "   - The remaining distribution is more representative of genuine writing\n",
    "   - Helps the model learn meaningful stylistic patterns rather than memorizing anomalies\n",
    "\n",
    "By removing these edge cases, we improve data quality while retaining the vast majority of valid examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lengths = (\n",
    "    pd.concat([\n",
    "        df[['sentence1']].rename(columns={'sentence1': 'sentence'}),\n",
    "        df[['sentence2']].rename(columns={'sentence2': 'sentence'})\n",
    "    ])\n",
    "    .drop_duplicates()\n",
    ")\n",
    "df_lengths['length'] = df_lengths['sentence'].str.len()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "sns.histplot(df_lengths['length'], bins=50, kde=True, ax=axes[0])\n",
    "axes[0].set_title(\"Initial Sentence Length Distribution\")\n",
    "axes[0].set_xlabel(\"Character Length\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "print(\"\\nInitial sentence length statistics:\")\n",
    "print(df_lengths['length'].describe(percentiles=[0.01, 0.025, 0.95, 0.99]))\n",
    "\n",
    "sns.histplot(df_lengths['length'], bins=50, kde=True, ax=axes[1])\n",
    "axes[1].set_title(\"Distribution with Cutoffs\")\n",
    "axes[1].axvline(x=df_lengths['length'].quantile(0.025), color='orange', linestyle='--', label='2.5th Percentile')\n",
    "axes[1].axvline(x=df_lengths['length'].quantile(0.99), color='red', linestyle='--', label='99th Percentile')\n",
    "axes[1].set_xlabel(\"Character Length\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Filter outliers\n",
    "upper_cutoff = df_lengths['length'].quantile(0.99)\n",
    "lower_cutoff = df_lengths['length'].quantile(0.025)\n",
    "print(f\"\\nRemoving sentences shorter than {lower_cutoff} and longer than {upper_cutoff} characters as outliers.\")\n",
    "\n",
    "df = df[\n",
    "    (df['sentence1'].str.len() >= lower_cutoff) &\n",
    "    (df['sentence1'].str.len() <= upper_cutoff) &\n",
    "    (df['sentence2'].str.len() >= lower_cutoff) &\n",
    "    (df['sentence2'].str.len() <= upper_cutoff)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Recalculate lengths after filtering\n",
    "df_lengths = (\n",
    "    pd.concat([\n",
    "        df[['sentence1']].rename(columns={'sentence1': 'sentence'}),\n",
    "        df[['sentence2']].rename(columns={'sentence2': 'sentence'})\n",
    "    ])\n",
    "    .drop_duplicates()\n",
    ")\n",
    "df_lengths['length'] = df_lengths['sentence'].str.len()\n",
    "\n",
    "sns.histplot(df_lengths['length'], bins=50, kde=True, ax=axes[2])\n",
    "axes[2].set_title(\"After Outlier Removal\")\n",
    "axes[2].set_xlabel(\"Character Length\")\n",
    "axes[2].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "label_counts = df['label'].value_counts()\n",
    "print(f\"\\nAfter this removal, the dataset is still imbalanced with {label_counts[False] / label_counts.sum():.2%} same-author pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing outliers (sentences below 2.5th percentile and above 99th percentile), even though we have made the underrepresented class less common, we have a cleaner distribution for model training.\n",
    "\n",
    "### Key Statistics After Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortest individual sentence\n",
    "shortest_len = min(df['sentence1'].str.len().min(), df['sentence2'].str.len().min())\n",
    "if df['sentence1'].str.len().min() <= df['sentence2'].str.len().min():\n",
    "    shortest_sentence = df.loc[df['sentence1'].str.len().idxmin(), 'sentence1']\n",
    "    shortest_type = \"sentence1\"\n",
    "else:\n",
    "    shortest_sentence = df.loc[df['sentence2'].str.len().idxmin(), 'sentence2']\n",
    "    shortest_type = \"sentence2\"\n",
    "\n",
    "print(f\"Shortest sentence length: {shortest_len}\")\n",
    "print(f\"Shortest sentence ({shortest_type}): {shortest_sentence}\\n\")\n",
    "\n",
    "# Longest individual sentence\n",
    "longest_sent1 = df['sentence1'].str.len().max()\n",
    "longest_sent2 = df['sentence2'].str.len().max()\n",
    "longest_len = max(longest_sent1, longest_sent2)\n",
    "if longest_sent1 >= longest_sent2:\n",
    "    longest_sentence = df.loc[df['sentence1'].str.len().idxmax(), 'sentence1']\n",
    "    longest_type = \"sentence1\"\n",
    "else:\n",
    "    longest_sentence = df.loc[df['sentence2'].str.len().idxmax(), 'sentence2']\n",
    "    longest_type = \"sentence2\"\n",
    "\n",
    "print(f\"Longest sentence length: {longest_len}\")\n",
    "print(f\"Longest sentence ({longest_type}): {longest_sentence}\\n\")\n",
    "\n",
    "# Longest combined sentence pair (sum of both sentences in the same row)\n",
    "df['combined_len'] = df['sentence1'].str.len() + df['sentence2'].str.len()\n",
    "longest_combined_idx = df['combined_len'].idxmax()\n",
    "longest_combined = df.loc[longest_combined_idx, 'combined_len']\n",
    "longest_combined_sent1 = df.loc[longest_combined_idx, 'sentence1']\n",
    "longest_combined_sent2 = df.loc[longest_combined_idx, 'sentence2']\n",
    "\n",
    "print(f\"Longest combined pair length: {longest_combined}\")\n",
    "print(f\"Sentence 1: {longest_combined_sent1}\")\n",
    "print(f\"Sentence 2: {longest_combined_sent2}\")\n",
    "\n",
    "print(f\"Mean sentence length: {df['combined_len'].mean():.1f} characters\")\n",
    "\n",
    "# Clean up temporary column\n",
    "df.drop('combined_len', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Tokenization Analysis\n",
    "\n",
    "Before building our models, we need to determine an appropriate `max_length` for our tokenizer. This involves analyzing the distribution of token lengths in our dataset to balance between:\n",
    "- **Capturing most of the content**: Not truncating too many sentences\n",
    "- **Computational efficiency**: Shorter sequences = faster training\n",
    "- **Memory constraints**: Longer sequences use more GPU memory and may cause more padding to be used, which is inefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use GPT-2 tokenizer for analysis as a representative tokenizer since it is the une we used in our custom model.\n",
    "analysis_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "analysis_tokenizer.pad_token = analysis_tokenizer.eos_token\n",
    "\n",
    "def get_token_length(text: str, tokenizer) -> int:\n",
    "    \"\"\"Get the number of tokens for a text string.\"\"\"\n",
    "    return len(tokenizer.encode(text, add_special_tokens=True))\n",
    "\n",
    "print(\"Analyzing token lengths...\")\n",
    "df['sent1_tokens'] = df['sentence1'].apply(lambda x: get_token_length(x, analysis_tokenizer))\n",
    "df['sent2_tokens'] = df['sentence2'].apply(lambda x: get_token_length(x, analysis_tokenizer))\n",
    "\n",
    "df['combined_tokens'] = df.apply(\n",
    "    lambda row: len(analysis_tokenizer.encode(\n",
    "        row['sentence1'], row['sentence2'], add_special_tokens=True\n",
    "    )), axis=1\n",
    ")\n",
    "\n",
    "df['max_single_tokens'] = df[['sent1_tokens', 'sent2_tokens']].max(axis=1)\n",
    "\n",
    "print(\"\\nStandard Mode (concatenated sentences):\")\n",
    "print(df['combined_tokens'].describe())\n",
    "\n",
    "print(\"\\nSiamese Mode (individual sentences):\")\n",
    "print(df['max_single_tokens'].describe())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df['combined_tokens'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(df['combined_tokens'].quantile(0.95), color='red', linestyle='--', \n",
    "                label=f\"95th percentile: {df['combined_tokens'].quantile(0.95):.0f}\")\n",
    "axes[0].axvline(df['combined_tokens'].quantile(0.99), color='orange', linestyle='--',\n",
    "                label=f\"99th percentile: {df['combined_tokens'].quantile(0.99):.0f}\")\n",
    "axes[0].set_xlabel('Token Length (Combined)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Standard Mode: Concatenated Token Lengths')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Siamese mode distribution\n",
    "axes[1].hist(df['max_single_tokens'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(df['max_single_tokens'].quantile(0.95), color='red', linestyle='--',\n",
    "                label=f\"95th percentile: {df['max_single_tokens'].quantile(0.95):.0f}\")\n",
    "axes[1].axvline(df['max_single_tokens'].quantile(0.99), color='orange', linestyle='--',\n",
    "                label=f\"99th percentile: {df['max_single_tokens'].quantile(0.99):.0f}\")\n",
    "axes[1].set_xlabel('Token Length (Max Single)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Siamese Mode: Individual Sentence Token Lengths')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "percentiles = [0.90, 0.95, 0.99, 1.0]\n",
    "print(\"\\nStandard Mode (concatenated):\")\n",
    "for p in percentiles:\n",
    "    val = df['combined_tokens'].quantile(p)\n",
    "    pct = p * 100\n",
    "    truncated = (df['combined_tokens'] > val).sum()\n",
    "    print(f\"  {pct:5.1f}th percentile: {val:6.0f} tokens ({truncated:,} samples would be truncated, {truncated/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nSiamese Mode (individual):\")\n",
    "for p in percentiles:\n",
    "    val = df['max_single_tokens'].quantile(p)\n",
    "    pct = p * 100\n",
    "    truncated = (df['max_single_tokens'] > val).sum()\n",
    "    print(f\"  {pct:5.1f}th percentile: {val:6.0f} tokens ({truncated:,} samples would be truncated, {truncated/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nWe will use max_length=128 as it captures 99%+ of sequences (only ~1% truncated) while maximizing efficiency.\")\n",
    "\n",
    "# Clean up temporary columns\n",
    "df = df.drop(columns=['sent1_tokens', 'sent2_tokens', 'combined_tokens', 'max_single_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Model Architecture\n",
    "\n",
    "## 2.1 Device Setup\n",
    "\n",
    "We check for GPU availability to accelerate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_MODEL_NAME = 'custom-lightweight-transformer'  # Name for the custom model\n",
    "SIAMESE_PREFIX = 'siamese-'  # Prefix for siamese model variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Encoder Architectures\n",
    "\n",
    "We define a base encoder class and two implementations:\n",
    "\n",
    "1. **LightweightTransformerEncoder**: A custom, small transformer trained from scratch\n",
    "2. **PretrainedEncoder**: A wrapper around HuggingFace pretrained models\n",
    "\n",
    "Both encoders output a fixed-size embedding for each input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEncoder(nn.Module, ABC):\n",
    "    \"\"\"Abstract base class for all encoders (custom and pretrained)\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def encode(self, input_ids, attention_mask=None):\n",
    "        \"\"\"Encode input and return pooled representation\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_output_dim(self) -> int:\n",
    "        \"\"\"Return the dimension of the encoder output\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def save_pretrained(self, path: str):\n",
    "        \"\"\"Save encoder to disk\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def from_pretrained(cls, path: str):\n",
    "        \"\"\"Load encoder from disk\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightweightTransformerEncoder(BaseEncoder):\n",
    "    \"\"\"\n",
    "    Custom lightweight transformer encoder.\n",
    "    \n",
    "    A small transformer that can be trained from scratch.\n",
    "    Uses learned positional embeddings and returns CLS token representation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers,\n",
    "                 dim_feedforward, max_length, dropout, pad_token_id):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_token_id)\n",
    "        self.position_embedding = nn.Embedding(max_length, d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, activation='gelu', batch_first=True, norm_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers, enable_nested_tensor=False)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.config = {\n",
    "            'vocab_size': vocab_size,\n",
    "            'd_model': d_model,\n",
    "            'nhead': nhead,\n",
    "            'num_layers': num_layers,\n",
    "            'dim_feedforward': dim_feedforward,\n",
    "            'max_length': max_length,\n",
    "            'dropout': dropout,\n",
    "            'pad_token_id': pad_token_id\n",
    "        }\n",
    "    \n",
    "    def encode(self, input_ids, attention_mask=None):\n",
    "        seq_len = min(input_ids.size(1), self.config['max_length'])\n",
    "        pos_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
    "        x = self.embedding(input_ids[:, :seq_len]) + self.position_embedding(pos_ids)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask[:, :seq_len]\n",
    "        \n",
    "        src_key_padding_mask = (attention_mask == 0) if attention_mask is not None else None\n",
    "        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        return x[:, 0, :]  # CLS token (first position)\n",
    "    \n",
    "    def get_output_dim(self) -> int:\n",
    "        return self.d_model\n",
    "    \n",
    "    def save_pretrained(self, path: str):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        torch.save(self.state_dict(), os.path.join(path, 'pytorch_model.bin'))\n",
    "        with open(os.path.join(path, 'config.json'), 'w') as f:\n",
    "            json.dump(self.config, f, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, path: str):\n",
    "        with open(os.path.join(path, 'config.json'), 'r') as f:\n",
    "            config = json.load(f)\n",
    "        model = cls(**config)\n",
    "        model.load_state_dict(torch.load(os.path.join(path, 'pytorch_model.bin'), weights_only=True))\n",
    "        return model\n",
    "    \n",
    "    def num_parameters(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedEncoder(BaseEncoder):\n",
    "    \"\"\"\n",
    "    Wrapper for HuggingFace pretrained models.\n",
    "    \n",
    "    Leverages pretrained weights from models like BERT, RoBERTa, etc.\n",
    "    Returns either pooler_output or CLS token embedding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model_name = model_name\n",
    "        self._output_dim = self.model.config.hidden_size\n",
    "    \n",
    "    def encode(self, input_ids, attention_mask=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "            return outputs.pooler_output\n",
    "        return outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "    \n",
    "    def get_output_dim(self) -> int:\n",
    "        return self._output_dim\n",
    "    \n",
    "    def save_pretrained(self, path: str):\n",
    "        self.model.save_pretrained(path)\n",
    "        with open(os.path.join(path, 'encoder_info.json'), 'w') as f:\n",
    "            json.dump({'model_name': self.model_name}, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, path: str):\n",
    "        instance = object.__new__(cls)\n",
    "        nn.Module.__init__(instance)\n",
    "        instance.model = AutoModel.from_pretrained(path)\n",
    "        instance._output_dim = instance.model.config.hidden_size\n",
    "        with open(os.path.join(path, 'encoder_info.json'), 'r') as f:\n",
    "            info = json.load(f)\n",
    "        instance.model_name = info['model_name']\n",
    "        return instance\n",
    "    \n",
    "    def num_parameters(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Classification Head\n",
    "\n",
    "A simple MLP classification head that takes encoder output and produces class logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"Simple MLP classification head with dropout regularization.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, dropout: float, num_labels: int = 2):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_dim // 2, num_labels)\n",
    "        )\n",
    "        self.config = {'input_dim': input_dim, 'num_labels': num_labels, 'dropout': dropout}\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "    \n",
    "    def save_pretrained(self, path: str):\n",
    "        torch.save(self.state_dict(), os.path.join(path, 'classifier.bin'))\n",
    "        with open(os.path.join(path, 'classifier_config.json'), 'w') as f:\n",
    "            json.dump(self.config, f, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, path: str):\n",
    "        with open(os.path.join(path, 'classifier_config.json'), 'r') as f:\n",
    "            config = json.load(f)\n",
    "        model = cls(**config)\n",
    "        model.load_state_dict(torch.load(os.path.join(path, 'classifier.bin'), weights_only=True))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Full Model Architectures\n",
    "\n",
    "### AuthorshipModel\n",
    "\n",
    "This model supports two modes:\n",
    "\n",
    "1. **Standard mode**: Takes concatenated sentence pairs as input (using tokenizer's SEP token)\n",
    "2. **Siamese mode**: Encodes each sentence separately and combines embeddings using configurable similarity methods:\n",
    "   - **concat**: Concatenation of both embeddings `[emb1, emb2]`\n",
    "   - **diff**: Absolute difference `|emb1 - emb2|` (captures element-wise dissimilarity)\n",
    "   - **mult**: Element-wise multiplication `emb1 * emb2` (captures feature interaction)\n",
    "   - **cos**: Cosine similarity (captures global angular alignment)\n",
    "\n",
    "The similarity methods can be combined in any configuration by passing a list (e.g., `['concat', 'diff', 'mult', 'cos']` for all features, or `['cos']` for just cosine similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorshipModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model for authorship verification.\n",
    "    \n",
    "    Can operate in standard mode (concatenated input) or siamese mode (separate encoding).\n",
    "    In siamese mode, embeddings are combined using the specified similarity methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encoder: BaseEncoder, dropout: float, num_labels: int = 2,\n",
    "                 siamese: bool = False, similarity_methods: list = ['concat', 'diff', 'mult', 'cos']):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.siamese = siamese\n",
    "        self.similarity_methods = similarity_methods if siamese else []  # Clear for non-siamese\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        encoder_dim = encoder.get_output_dim()\n",
    "        \n",
    "        if siamese:\n",
    "            classifier_input = 0\n",
    "            \n",
    "            if 'concat' in self.similarity_methods:\n",
    "                classifier_input += encoder_dim * 2\n",
    "            if 'diff' in self.similarity_methods:\n",
    "                classifier_input += encoder_dim\n",
    "            if 'mult' in self.similarity_methods:\n",
    "                classifier_input += encoder_dim\n",
    "            if 'cos' in self.similarity_methods:\n",
    "                classifier_input += 1\n",
    "            \n",
    "            if classifier_input == 0:\n",
    "                raise ValueError(\"At least one similarity method must be specified for siamese mode\")\n",
    "        else:\n",
    "            classifier_input = encoder_dim\n",
    "        \n",
    "        self.classifier = ClassificationHead(classifier_input, num_labels=num_labels, dropout=dropout)\n",
    "        \n",
    "        self.config = {\n",
    "            'num_labels': num_labels,\n",
    "            'dropout': dropout,\n",
    "            'siamese': siamese,\n",
    "            'similarity_methods': self.similarity_methods\n",
    "        }\n",
    "    \n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2=None, attention_mask2=None, \n",
    "                labels=None, class_weights=None, label_smoothing=0.0):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            input_ids1, attention_mask1: Primary input (or concatenated sentences in standard mode)\n",
    "            input_ids2, attention_mask2: Second sentence (siamese mode only)\n",
    "            labels: Ground truth labels for loss computation\n",
    "            class_weights: Optional weights for imbalanced classes\n",
    "            label_smoothing: Label smoothing factor for regularization\n",
    "        \"\"\"\n",
    "        if self.siamese:\n",
    "            if input_ids2 is None or attention_mask2 is None:\n",
    "                raise ValueError(\"Siamese mode requires input_ids2 and attention_mask2\")\n",
    "            \n",
    "            emb1 = self.encoder.encode(input_ids1, attention_mask1)\n",
    "            emb2 = self.encoder.encode(input_ids2, attention_mask2)\n",
    "            \n",
    "            features = []\n",
    "            \n",
    "            if 'concat' in self.similarity_methods:\n",
    "                features.extend([emb1, emb2])\n",
    "            \n",
    "            if 'diff' in self.similarity_methods:\n",
    "                diff = torch.abs(emb1 - emb2)\n",
    "                features.append(diff)\n",
    "            \n",
    "            if 'mult' in self.similarity_methods:\n",
    "                mult = emb1 * emb2\n",
    "                features.append(mult)\n",
    "            \n",
    "            if 'cos' in self.similarity_methods:\n",
    "                cos_sim = F.cosine_similarity(emb1, emb2, dim=-1).unsqueeze(-1)\n",
    "                features.append(cos_sim)\n",
    "            \n",
    "            combined = torch.cat(features, dim=-1)\n",
    "            logits = self.classifier(combined)\n",
    "        else:\n",
    "            emb = self.encoder.encode(input_ids1, attention_mask1)\n",
    "            logits = self.classifier(emb)\n",
    "        \n",
    "        output = type('Output', (), {'logits': logits})()\n",
    "        \n",
    "        if labels is not None:\n",
    "            output.loss = F.cross_entropy(logits, labels, weight=class_weights, label_smoothing=label_smoothing)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def save_pretrained(self, path: str):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        self.encoder.save_pretrained(os.path.join(path, 'encoder'))\n",
    "        self.classifier.save_pretrained(path)\n",
    "        with open(os.path.join(path, 'model_config.json'), 'w') as f:\n",
    "            json.dump(self.config, f, indent=2)\n",
    "        print(f\"Model saved to {path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, path: str, encoder_class: BaseEncoder):\n",
    "        with open(os.path.join(path, 'model_config.json'), 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        encoder = encoder_class.from_pretrained(os.path.join(path, 'encoder'))\n",
    "        model = cls(encoder, **config)\n",
    "        model.classifier = ClassificationHead.from_pretrained(path)\n",
    "        return model\n",
    "    \n",
    "    def num_parameters(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Dataset Classes\n",
    "\n",
    "We define two dataset classes:\n",
    "\n",
    "1. **StandardDataset**: Tokenizes sentence pairs together\n",
    "2. **SiameseDataset**: Tokenizes each sentence separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardDataset(Dataset):\n",
    "    \"\"\"Dataset that tokenizes sentence pairs together (for non-siamese models).\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.df = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            row['sentence1'], row['sentence2'],\n",
    "            padding='max_length', truncation=True,\n",
    "            max_length=self.max_length, return_tensors='pt'\n",
    "        )\n",
    "        return (\n",
    "            encoding['input_ids'].squeeze(0),\n",
    "            encoding['attention_mask'].squeeze(0),\n",
    "            torch.tensor(int(row['label']), dtype=torch.long)\n",
    "        )\n",
    "\n",
    "\n",
    "class SiameseDataset(Dataset):\n",
    "    \"\"\"Dataset that tokenizes each sentence separately (for siamese models).\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.df = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        enc1 = self.tokenizer(row['sentence1'], padding='max_length', truncation=True,\n",
    "                              max_length=self.max_length, return_tensors='pt')\n",
    "        enc2 = self.tokenizer(row['sentence2'], padding='max_length', truncation=True,\n",
    "                              max_length=self.max_length, return_tensors='pt')\n",
    "        return (\n",
    "            enc1['input_ids'].squeeze(0), enc1['attention_mask'].squeeze(0),\n",
    "            enc2['input_ids'].squeeze(0), enc2['attention_mask'].squeeze(0),\n",
    "            torch.tensor(int(row['label']), dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Model Configuration\n",
    "\n",
    "The `ModelConfig` dataclass and `get_model_config` function provide an interface for creating and configuring different model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size_mb(model: nn.Module, assume_uniform_dtype: bool = True) -> float:\n",
    "    \"\"\"Calculate actual model size in MB based on parameter dtypes.\"\"\"\n",
    "    params = list(model.parameters())\n",
    "    if not params:\n",
    "        return 0.0\n",
    "    \n",
    "    if assume_uniform_dtype:\n",
    "        return (model.num_parameters() * params[0].element_size()) / 1e6\n",
    "    \n",
    "    total_bytes = 0\n",
    "    for param in model.parameters():\n",
    "        total_bytes += param.numel() * param.element_size()\n",
    "    return total_bytes / 1e6\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration container for a model.\"\"\"\n",
    "    model_name: str\n",
    "    model: AuthorshipModel\n",
    "    device: torch.device\n",
    "    tokenizer: AutoTokenizer\n",
    "    max_length: int\n",
    "    batch_size: int\n",
    "    learning_rate: float\n",
    "    num_epochs: int\n",
    "    model_path: str\n",
    "    is_siamese: bool\n",
    "    is_custom: bool\n",
    "    dataset_class: type  # Either StandardDataset or SiameseDataset\n",
    "\n",
    "def get_model_config(\n",
    "    model_name: str,\n",
    "    device: torch.device,\n",
    "    max_length: int = 128,  # Value chosen based on previous analysis\n",
    "    num_labels: int = 2,\n",
    "    # Custom model parameters\n",
    "    custom_d_model: int = 128,\n",
    "    custom_nhead: int = 4,\n",
    "    custom_num_layers: int = 2,\n",
    "    custom_dim_feedforward: int = 256,\n",
    "    dropout: float = 1/3,  # 0,33... dropout\n",
    "    # Siamese parameters\n",
    "    siamese_similarity_methods: list[str] = ['concat', 'diff', 'mult', 'cos'],\n",
    "    # Training parameters\n",
    "    batch_size: int = None,\n",
    "    learning_rate: float = None,\n",
    "    num_epochs: int = None\n",
    ") -> ModelConfig:\n",
    "    \"\"\"\n",
    "    Creates model configuration based on model name. For custom models, it uses gpt2's tokenizer.\n",
    "    \n",
    "    Supported formats:\n",
    "        - 'custom-lightweight-transformer': Custom transformer\n",
    "        - 'siamese-custom-lightweight-transformer': Siamese custom transformer  \n",
    "    It also supports pretrained HuggingFace models with and without 'siamese-' prefix:\n",
    "        - 'prajjwal1/bert-mini'\n",
    "        - 'microsoft/deberta-v3-small'\n",
    "        - 'Qwen/Qwen2.5-0.5B'\n",
    "    \"\"\"\n",
    "    if (not batch_size) or (not learning_rate) or (not num_epochs):\n",
    "        print(\"WARNING: One or more training parameters (batch_size, learning_rate, num_epochs) were not provided in the get_model_config function and will be set to default values.\")\n",
    "    \n",
    "    if model_name.endswith(CUSTOM_MODEL_NAME):\n",
    "        if not batch_size:\n",
    "            batch_size = 64\n",
    "        if not learning_rate:\n",
    "            learning_rate = 1e-3\n",
    "        if not num_epochs:\n",
    "            num_epochs = 20  # 10 may be enough\n",
    "    else:\n",
    "        if not batch_size:\n",
    "            batch_size = 16\n",
    "        if not learning_rate:\n",
    "            learning_rate = 2e-5  # maybe even 1e-5\n",
    "        if not num_epochs:\n",
    "            num_epochs = 10  # 5 may be enough\n",
    "    is_siamese = model_name.startswith(SIAMESE_PREFIX)\n",
    "    base_model_name = model_name[len(SIAMESE_PREFIX):] if is_siamese else model_name\n",
    "    is_custom = base_model_name == CUSTOM_MODEL_NAME\n",
    "    \n",
    "    model_path = os.path.join('.', 'results', model_name.replace(\"/\", \"-\"))\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Type: {'Siamese ' if is_siamese else ''}{'Custom' if is_custom else 'Pretrained'}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if is_custom:\n",
    "        tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        encoder = LightweightTransformerEncoder(\n",
    "            vocab_size=len(tokenizer),\n",
    "            d_model=custom_d_model,\n",
    "            nhead=custom_nhead,\n",
    "            num_layers=custom_num_layers,\n",
    "            dim_feedforward=custom_dim_feedforward,\n",
    "            max_length=max_length,\n",
    "            dropout=dropout,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        encoder = PretrainedEncoder(base_model_name)\n",
    "\n",
    "    model = AuthorshipModel(\n",
    "        encoder=encoder,\n",
    "        num_labels=num_labels,\n",
    "        dropout=dropout,\n",
    "        siamese=is_siamese,\n",
    "        similarity_methods=siamese_similarity_methods\n",
    "    ).to(device)\n",
    "    \n",
    "    dataset_class = SiameseDataset if is_siamese else StandardDataset\n",
    "    \n",
    "    print(f\"Parameters: {model.num_parameters():,}\")\n",
    "    print(f\"Size: ~{get_model_size_mb(model):.2f} MB\")\n",
    "    \n",
    "    model.encoder\n",
    "    return ModelConfig(\n",
    "        model_name=model_name,\n",
    "        model=model,\n",
    "        device=device,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        num_epochs=num_epochs,\n",
    "        model_path=model_path,\n",
    "        is_siamese=is_siamese,\n",
    "        is_custom=is_custom,\n",
    "        dataset_class=dataset_class\n",
    "    )\n",
    "\n",
    "def load_model_from_config(config: ModelConfig, move_to_device: bool = True) -> nn.Module:\n",
    "    \"\"\"Load a model from disk using its config.\"\"\"\n",
    "    path = config.model_path\n",
    "    \n",
    "    model = AuthorshipModel.from_pretrained(path, type(config.model.encoder))\n",
    "    \n",
    "    if move_to_device:\n",
    "        model = model.to(config.device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Architecture Overview\n",
    "\n",
    "The modular design we applied allows for flexible experimentation with different encoder types (custom vs. pretrained) and training modes (standard vs. siamese).\n",
    "\n",
    "The following cell should be rendered as a class diagram of the model architecture. If it does not, you can visualize it here: [https://mermaid.live/](https://mermaid.live/)\n",
    "\n",
    "If you are using Visual Studio Code, after installing [Markdown Preview Mermaid Support](https://marketplace.visualstudio.com/items?itemName=bierner.markdown-mermaid) you should be able to visualize it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "classDiagram\n",
    "    class BaseEncoder {\n",
    "        <<abstract>>\n",
    "        +encode(input_ids, attention_mask)*\n",
    "        +get_output_dim()* int\n",
    "        +save_pretrained(path)*\n",
    "        +from_pretrained(path)*\n",
    "    }\n",
    "    \n",
    "    class LightweightTransformerEncoder {\n",
    "        -embedding: nn.Embedding\n",
    "        -position_embedding: nn.Embedding\n",
    "        -transformer: nn.TransformerEncoder\n",
    "        -d_model: int\n",
    "        -config: dict\n",
    "        +encode(input_ids, attention_mask)\n",
    "        +get_output_dim() int\n",
    "        +save_pretrained(path)\n",
    "        +from_pretrained(path)\n",
    "        +num_parameters() int\n",
    "    }\n",
    "    \n",
    "    class PretrainedEncoder {\n",
    "        -model: AutoModel\n",
    "        -model_name: str\n",
    "        -_output_dim: int\n",
    "        +encode(input_ids, attention_mask)\n",
    "        +get_output_dim() int\n",
    "        +save_pretrained(path)\n",
    "        +from_pretrained(path)\n",
    "        +num_parameters() int\n",
    "    }\n",
    "    \n",
    "    class ClassificationHead {\n",
    "        -classifier: nn.Sequential\n",
    "        -config: dict\n",
    "        +forward(x)\n",
    "        +save_pretrained(path)\n",
    "        +from_pretrained(path)\n",
    "    }\n",
    "    \n",
    "    class AuthorshipModel {\n",
    "        -encoder: BaseEncoder\n",
    "        -classifier: ClassificationHead\n",
    "        -siamese: bool\n",
    "        -similarity_methods: list[str]\n",
    "        -num_labels: int\n",
    "        -config: dict\n",
    "        +forward(input_ids1, attention_mask1, ...)\n",
    "        +save_pretrained(path)\n",
    "        +from_pretrained(path, encoder_class)\n",
    "        +num_parameters() int\n",
    "    }\n",
    "    \n",
    "    class ModelConfig {\n",
    "        <<dataclass>>\n",
    "        +model_name: str\n",
    "        +model: AuthorshipModel\n",
    "        +device: torch.device\n",
    "        +tokenizer: AutoTokenizer\n",
    "        +max_length: int\n",
    "        +batch_size: int\n",
    "        +learning_rate: float\n",
    "        +num_epochs: int\n",
    "        +model_path: str\n",
    "        +is_siamese: bool\n",
    "        +is_custom: bool\n",
    "        +dataset_class: type\n",
    "    }\n",
    "    \n",
    "    class Dataset {\n",
    "        <<abstract>>\n",
    "        +__len__()*\n",
    "        +__getitem__(idx)*\n",
    "    }\n",
    "    \n",
    "    class StandardDataset {\n",
    "        -df: DataFrame\n",
    "        -tokenizer: AutoTokenizer\n",
    "        -max_length: int\n",
    "        +__len__() int\n",
    "        +__getitem__(idx) tuple[Tensor, Tensor, Tensor]\n",
    "    }\n",
    "    \n",
    "    class SiameseDataset {\n",
    "        -df: DataFrame\n",
    "        -tokenizer: AutoTokenizer\n",
    "        -max_length: int\n",
    "        +__len__() int\n",
    "        +__getitem__(idx) tuple[Tensor, Tensor, Tensor, Tensor, Tensor]\n",
    "    }\n",
    "    \n",
    "    class TrainingResult {\n",
    "        <<dataclass>>\n",
    "        +best_f1: float\n",
    "        +best_epoch: int\n",
    "        +final_epoch: int\n",
    "        +training_history: list\n",
    "    }\n",
    "    \n",
    "    BaseEncoder <|-- LightweightTransformerEncoder : implements\n",
    "    BaseEncoder <|-- PretrainedEncoder : implements\n",
    "    \n",
    "    AuthorshipModel *-- BaseEncoder : contains\n",
    "    AuthorshipModel *-- ClassificationHead : contains\n",
    "\n",
    "    AuthorshipModel ..> TrainingResult : training produces\n",
    "    Dataset ..> TrainingResult : used to generate\n",
    "\n",
    "    Dataset <|-- StandardDataset : implements\n",
    "    Dataset <|-- SiameseDataset : implements\n",
    "\n",
    "    ModelConfig --> AuthorshipModel : references\n",
    "    ModelConfig --> Dataset : references via dataset_class\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Training\n",
    "\n",
    "## 3.1 Evaluation Metrics\n",
    "\n",
    "We use several metrics to evaluate model performance:\n",
    "\n",
    "- **Accuracy**: Overall correctness\n",
    "- **Precision**: Of predicted positives, how many are correct\n",
    "- **Recall**: Of actual positives, how many were detected\n",
    "- **F1 Score**: Harmonic mean of precision and recall (our primary metric)\n",
    "- **AUC-ROC**: Area under the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(all_preds, all_labels, all_probs):\n",
    "    \"\"\"Compute classification metrics.\"\"\"\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='binary', zero_division=0\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        auc_roc = roc_auc_score(all_labels, all_probs)\n",
    "    except (ValueError, AttributeError) as e:\n",
    "        print(f\"Warning: Could not compute AUC-ROC ({type(e).__name__}): {e}\")\n",
    "        auc_roc = 0.0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc_roc': auc_roc\n",
    "    }\n",
    "\n",
    "def evaluate(model: AuthorshipModel, dataloader: DataLoader, device: torch.device):\n",
    "    \"\"\"Evaluate model on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = [b.to(device) for b in batch]\n",
    "            \n",
    "            if model.siamese:\n",
    "                input_ids1, attention_mask1, input_ids2, attention_mask2, labels = batch\n",
    "                outputs = model(input_ids1, attention_mask1, input_ids2, attention_mask2, labels=labels)\n",
    "            else:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                outputs = model(input_ids, attention_mask, labels=labels)\n",
    "            \n",
    "            total_loss += outputs.loss.item()\n",
    "            \n",
    "            probs = F.softmax(outputs.logits, dim=-1)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "    \n",
    "    metrics = compute_metrics(all_preds, all_labels, all_probs)\n",
    "    metrics['loss'] = total_loss / len(dataloader)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Training Function\n",
    "\n",
    "The training function includes:\n",
    "\n",
    "- **Weighted sampling** (default: enabled) to handle class imbalance by oversampling minority class examples during batch creation\n",
    "- **Weighted loss** (default: disabled) to handle class imabalance by assigning higher penalty to misclassifications of the underrepresented class\n",
    "- **Label smoothing** (default: 0.1) for regularization\n",
    "- **OneCycleLR scheduler** for learning rate scheduling\n",
    "- **Encoder freezing** (default: disabled) to support progressive fine-tuning:\n",
    "  - `False`: train all parameters from the start (full fine-tuning)\n",
    "  - `True`: freeze encoder completely (feature extraction mode)\n",
    "  - `float` (0-1): freeze encoder for that fraction of epochs, then unfreeze (progressive fine-tuning)\n",
    "- **Early stopping** to prevent overfitting\n",
    "- **Gradient clipping** for training stability\n",
    "\n",
    "Weighted loss is disabled by default since weighted sampling already addresses class imbalance, and combining both techniques can over-correct and hurt performance on the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingResult:\n",
    "    \"\"\"Results from training a model.\"\"\"\n",
    "    best_f1: float\n",
    "    best_epoch: int\n",
    "    final_epoch: int\n",
    "    training_history: list = field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    config: ModelConfig,\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    max_patience: int = 3,\n",
    "    weight_decay: float = 0.1,\n",
    "    label_smoothing: float = 0.1,\n",
    "    weighted_sampler: bool = True,\n",
    "    weighted_loss: bool = False,\n",
    "    freeze_encoder: bool | float = False\n",
    ") -> tuple[nn.Module, TrainingResult]:\n",
    "    \"\"\"\n",
    "    Train a model with early stopping and various regularization techniques.\n",
    "    \n",
    "    Parameters:\n",
    "        config: ModelConfig object containing model configuration\n",
    "        train_df, val_df: DataFrames with 'sentence1', 'sentence2', 'label' columns\n",
    "        max_patience: Early stopping patience (epochs without improvement)\n",
    "        weight_decay: L2 regularization strength\n",
    "        label_smoothing: Label smoothing factor for loss\n",
    "        weighted_sampler: Whether to use weighted random sampler for imbalanced data\n",
    "        weighted_loss: Whether to use class-weighted loss function\n",
    "        freeze_encoder: If True, freeze encoder weights; if float, initially freeze for that fraction of epochs\n",
    "    Returns:\n",
    "        tuple[nn.Module, TrainingResult]: Best model and training results\n",
    "    \"\"\"\n",
    "    \n",
    "    PCT_STEPS_FOR_WARMUP = 0.1  # During the first 10% of steps, LR increases from 0 to max_lr\n",
    "    \n",
    "    # Epoch in which to unfreeze encoder\n",
    "    unfreeze_at_epoch = 0\n",
    "    if isinstance(freeze_encoder, float):\n",
    "        if not (0.0 < freeze_encoder < 1.0):\n",
    "            raise ValueError(\"If freeze_encoder is a float, it must be between 0.0 and 1.0\")\n",
    "        if freeze_encoder < PCT_STEPS_FOR_WARMUP:\n",
    "            print(f\"WARNING: freeze_encoder fraction {freeze_encoder} is less than the warm-up percentage {PCT_STEPS_FOR_WARMUP}. This may lead to suboptimal training since the learning rate will not reach its maximum peak.\")\n",
    "        unfreeze_at_epoch = int(config.num_epochs * freeze_encoder)\n",
    "        print(f\"Encoder frozen until epoch {unfreeze_at_epoch + 1}/{config.num_epochs}\")\n",
    "    elif freeze_encoder is True:\n",
    "        unfreeze_at_epoch = float('inf')  # never unfreeze\n",
    "        print(f\"Encoder frozen for all epochs\")\n",
    "\n",
    "    model = config.model\n",
    "    best_model = model\n",
    "    tokenizer = config.tokenizer\n",
    "    \n",
    "    # Compute class weights for imbalanced data\n",
    "    labels = train_df['label'].astype(int).values\n",
    "    class_counts = np.bincount(labels)\n",
    "\n",
    "    if weighted_loss:\n",
    "        class_weights = torch.tensor(len(labels) / (2 * class_counts), dtype=torch.float32).to(config.device)\n",
    "    else:\n",
    "        class_weights = None\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = config.dataset_class(train_df, tokenizer, config.max_length)\n",
    "    val_dataset = config.dataset_class(val_df, tokenizer, config.max_length)\n",
    "\n",
    "    # Create data loaders with optional weighted sampling\n",
    "    if weighted_sampler:\n",
    "        sample_weights = 1.0 / class_counts[labels]\n",
    "        sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, sampler=sampler, num_workers=NUM_WORKERS)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    # Handle encoder freezing\n",
    "    if freeze_encoder:\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=config.learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    total_steps = len(train_loader) * config.num_epochs\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=config.learning_rate,  # Peak learning rate\n",
    "        total_steps=total_steps,\n",
    "        pct_start=PCT_STEPS_FOR_WARMUP,  # Warm-up for first 10% of steps, then decay\n",
    "        anneal_strategy='cos'  # Softer function compared to linear\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining {config.model_name}\")\n",
    "    print(f\"Train: {len(train_df):,}, Val: {len(val_df):,}\")\n",
    "    print(f\"Batch size: {config.batch_size}, LR: {config.learning_rate}, Epochs: {config.num_epochs}\")\n",
    "    print(f\"Total steps: {total_steps:,}\")\n",
    "    print(f\"Weighted sampler: {weighted_sampler}, Weighted loss: {weighted_loss}, Label smoothing: {label_smoothing}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Save initial model and tokenizer so that even if training is unsuccessful, we have something saved\n",
    "    os.makedirs(config.model_path, exist_ok=True)\n",
    "    model.save_pretrained(config.model_path)\n",
    "    tokenizer.save_pretrained(config.model_path)\n",
    "    \n",
    "    # Training loop\n",
    "    best_f1 = 0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    training_history = []\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        # if the encoder was frozen, unfreeze it after the specified number of epochs\n",
    "        if epoch == unfreeze_at_epoch:\n",
    "            print(f\"\\nUnfreezing encoder at epoch {epoch+1}/{config.num_epochs}\")\n",
    "            for param in model.encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "            optimizer = torch.optim.AdamW(\n",
    "                filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                lr=current_lr,  # make sure to continue from current LR so that there is no spike in LR\n",
    "                weight_decay=weight_decay\n",
    "            )\n",
    "            \n",
    "            remaining_steps = len(train_loader) * (config.num_epochs - epoch)\n",
    "            # now only using cosine annealing for the remaining steps to smoothly decay the LR (no increase)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                T_max=remaining_steps,\n",
    "                eta_min=current_lr * 0.1  # Minimum = 10% of current LR\n",
    "            )\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_labels_list = [], []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            batch = [b.to(config.device) for b in batch]\n",
    "            \n",
    "            if config.is_siamese:\n",
    "                input_ids1, attention_mask1, input_ids2, attention_mask2, labels = batch\n",
    "                outputs = model(input_ids1, attention_mask1, input_ids2, attention_mask2, \n",
    "                               labels=labels, class_weights=class_weights, label_smoothing=label_smoothing)\n",
    "            else:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                outputs = model(input_ids, attention_mask, \n",
    "                               labels=labels, class_weights=class_weights, label_smoothing=label_smoothing)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs.loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels_list.extend(labels.cpu().numpy())\n",
    "            \n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(f\"  Epoch {epoch+1} - Batch {batch_idx+1}/{len(train_loader)} - Loss: {outputs.loss.item():.4f}\")\n",
    "        \n",
    "        train_acc = accuracy_score(train_labels_list, train_preds)\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        val_metrics = evaluate(model, val_loader, config.device)\n",
    "        \n",
    "        epoch_record = {\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            **{f'val_{k}': v for k, v in val_metrics.items()}\n",
    "        }\n",
    "        training_history.append(epoch_record)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{config.num_epochs} stats\")\n",
    "        print(f\"  Train - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val   - Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.4f}, \"\n",
    "              f\"P: {val_metrics['precision']:.4f}, R: {val_metrics['recall']:.4f}, \"\n",
    "              f\"F1: {val_metrics['f1']:.4f}, AUC: {val_metrics['auc_roc']:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_metrics['f1'] > best_f1:\n",
    "            best_f1 = val_metrics['f1']\n",
    "            best_epoch = epoch + 1\n",
    "            patience_counter = 0\n",
    "            \n",
    "            model.save_pretrained(config.model_path)\n",
    "            best_model = model\n",
    "            print(f\"  New best model saved (F1: {best_f1:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= max_patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        print()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Training complete! Best F1: {best_f1:.4f} at epoch {best_epoch}\")\n",
    "    print(f\"Best model saved to: {config.model_path}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return (\n",
    "        best_model,\n",
    "        TrainingResult(\n",
    "            best_f1=best_f1,\n",
    "            best_epoch=best_epoch,\n",
    "            final_epoch=epoch + 1,\n",
    "            training_history=training_history\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Model Comparison Function\n",
    "\n",
    "This function trains and evaluates multiple models, producing a comparison table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(\n",
    "    model_names: list[str],\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    model_config_kwargs: dict = None,\n",
    "    train_kwargs: dict = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Train and compare multiple models.\n",
    "    \n",
    "    Parameters:\n",
    "        model_names: List of model names to compare\n",
    "        train_df, val_df, test_df: DataFrames with 'sentence1', 'sentence2', 'label' columns\n",
    "        model_config_kwargs: Dict with optional 'pretrained' and/or 'custom' keys that provide kwargs for the specific model types\n",
    "        train_kwargs: Dict with optional 'pretrained' and/or 'custom' keys that provide kwargs for the specific model types\n",
    "    Returns:\n",
    "        DataFrame with comparison results sorted by F1 score\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, model_name in enumerate(model_names):\n",
    "        print(f\"\\n{'#' * 70}\")\n",
    "        print(f\"# Model {i+1}/{len(model_names)}: {model_name}\")\n",
    "        print(f\"{'#' * 70}\\n\")\n",
    "        \n",
    "        try:\n",
    "            is_custom = model_name.endswith(CUSTOM_MODEL_NAME)\n",
    "            model_type = 'custom' if is_custom else 'pretrained'\n",
    "            \n",
    "            config_kwargs = {}\n",
    "            if model_config_kwargs:\n",
    "                config_kwargs = model_config_kwargs.get(model_type, model_config_kwargs.get('default', {}))\n",
    "            \n",
    "            config = get_model_config(model_name, device, **config_kwargs)\n",
    "            \n",
    "            # Try to load existing model or train new one\n",
    "            try:\n",
    "                if (not ALWAYS_TRAIN) and os.path.exists(config.model_path):\n",
    "                    print(f\"\\nLoading trained model from {config.model_path}...\")\n",
    "                    model = load_model_from_config(config)\n",
    "                else:\n",
    "                    raise FileNotFoundError\n",
    "            except Exception as e:\n",
    "                if isinstance(e, FileNotFoundError):\n",
    "                    print(f\"Model not found at {config.model_path}.\")\n",
    "                else:\n",
    "                    print(f\"Could not load model: {e}\")\n",
    "\n",
    "                print(f\"\\nTraining model {model_name} from scratch...\")\n",
    "\n",
    "                current_train_kwargs = {}\n",
    "                if train_kwargs:\n",
    "                    current_train_kwargs = train_kwargs.get(model_type, train_kwargs.get('default', {}))\n",
    "                \n",
    "                model, _ = train_model(\n",
    "                    config=config,\n",
    "                    train_df=train_df,\n",
    "                    val_df=val_df,\n",
    "                    **current_train_kwargs\n",
    "                )\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            test_dataset = config.dataset_class(test_df, config.tokenizer, config.max_length)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "            test_metrics = evaluate(model, test_loader, config.device)\n",
    "            \n",
    "            results.append({\n",
    "                'model': model_name,\n",
    "                'test_accuracy': test_metrics['accuracy'],\n",
    "                'test_precision': test_metrics['precision'],\n",
    "                'test_recall': test_metrics['recall'],\n",
    "                'test_f1': test_metrics['f1'],\n",
    "                'test_auc_roc': test_metrics['auc_roc'],\n",
    "                'model_path': config.model_path,\n",
    "                'status': 'success'\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR training {model_name}: {e}\")\n",
    "            results.append({\n",
    "                'model': model_name,\n",
    "                'status': 'failed',\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Prediction Function\n",
    "\n",
    "A helper function to make predictions on individual sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_authorship_change(\n",
    "    sentence1: str,\n",
    "    sentence2: str,\n",
    "    model: AuthorshipModel,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    device: torch.device,\n",
    "    max_length: int\n",
    ") -> tuple[bool, float]:\n",
    "    \"\"\"\n",
    "    Predict whether two sentences were written by different authors.\n",
    "    \n",
    "    Returns:\n",
    "        (prediction, confidence): True if different authors, with confidence score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    if model.siamese:\n",
    "        enc1 = tokenizer(sentence1, padding='max_length', truncation=True,\n",
    "                        max_length=max_length, return_tensors='pt')\n",
    "        enc2 = tokenizer(sentence2, padding='max_length', truncation=True,\n",
    "                        max_length=max_length, return_tensors='pt')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                enc1['input_ids'].to(device), enc1['attention_mask'].to(device),\n",
    "                enc2['input_ids'].to(device), enc2['attention_mask'].to(device)\n",
    "            )\n",
    "    else:\n",
    "        enc = tokenizer(sentence1, sentence2, padding='max_length', truncation=True,\n",
    "                       max_length=max_length, return_tensors='pt')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(enc['input_ids'].to(device), enc['attention_mask'].to(device))\n",
    "    \n",
    "    probs = F.softmax(outputs.logits, dim=-1)\n",
    "    prediction = torch.argmax(probs, dim=-1).item()\n",
    "    confidence = probs[0][prediction].item()\n",
    "    \n",
    "    return bool(prediction), confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Data Splitting\n",
    "\n",
    "We split the data into train (70%), validation (15%), and test (15%) sets with stratification to maintain class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.7\n",
    "VALIDATION_TEST_RATIO = 0.5\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, train_size=TRAIN_RATIO, random_state=RANDOM_SEED, stratify=df['label']\n",
    ")\n",
    "validation_df, test_df = train_test_split(\n",
    "    temp_df, train_size=VALIDATION_TEST_RATIO, random_state=RANDOM_SEED, stratify=temp_df['label']\n",
    ")\n",
    "\n",
    "print(\"Dataset Splits:\")\n",
    "print(f\"  Training:   {len(train_df):,} samples ({train_df['label'].mean():.2%} positive)\")\n",
    "print(f\"  Validation: {len(validation_df):,} samples ({validation_df['label'].mean():.2%} positive)\")\n",
    "print(f\"  Test:       {len(test_df):,} samples ({test_df['label'].mean():.2%} positive)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Experiments\n",
    "\n",
    "## 4.1 Train a Single Model\n",
    "\n",
    "Let's train our custom lightweight transformer model and visualize the training progress. To change the model, you would just need to change the `model_name` variable with any of the names in the `get_model_config` function description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f'{prefix}{name}' where prefix can be nothing or SIAMESE_PREFIX and name can be CUSTOM_MODEL_NAME , 'prajjwal1/bert-mini', etc.\n",
    "model_name = CUSTOM_MODEL_NAME\n",
    "\n",
    "config = get_model_config(model_name, device)\n",
    "\n",
    "# Load existing model or train new one\n",
    "try:\n",
    "    if (not ALWAYS_TRAIN) and os.path.exists(config.model_path):\n",
    "        print(f\"\\nLoading trained model from {config.model_path}...\")\n",
    "        model = load_model_from_config(config)\n",
    "        result = None\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "except Exception as e:\n",
    "    if isinstance(e, FileNotFoundError):\n",
    "        print(f\"Model not found at {config.model_path}\")\n",
    "    else:\n",
    "        print(f\"Could not load model: {e}\")\n",
    "    print(f\"\\nTraining model {model_name} from scratch...\")\n",
    "    model, result = train_model(\n",
    "        config=config,\n",
    "        train_df=train_df,\n",
    "        val_df=validation_df\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history if we just trained\n",
    "if result is not None:\n",
    "    history_df = pd.DataFrame(result.training_history)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(history_df['epoch'], history_df['train_loss'], 'b-o', label='Train Loss')\n",
    "    axes[0].plot(history_df['epoch'], history_df['val_loss'], 'r-o', label='Validation Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Metrics plot\n",
    "    axes[1].plot(history_df['epoch'], history_df['val_accuracy'], 'g-o', label='Accuracy')\n",
    "    axes[1].plot(history_df['epoch'], history_df['val_f1'], 'm-o', label='F1 Score')\n",
    "    axes[1].plot(history_df['epoch'], history_df['val_precision'], 'c-o', label='Precision')\n",
    "    axes[1].plot(history_df['epoch'], history_df['val_recall'], 'y-o', label='Recall')\n",
    "    axes[1].axvline(x=result.best_epoch, color='red', linestyle='--', alpha=0.7, label=f'Best Epoch ({result.best_epoch})')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Score')\n",
    "    axes[1].set_title('Validation Metrics')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nBest F1: {result.best_f1:.4f} at epoch {result.best_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating model on test set...\")\n",
    "test_dataset = config.dataset_class(test_df, config.tokenizer, config.max_length)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "model = load_model_from_config(config)\n",
    "test_metrics = evaluate(model, test_loader, device)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nModel: {config.model_name}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "for key in ['accuracy', 'precision', 'recall', 'f1', 'auc_roc']:\n",
    "    print(f\"  {key.replace('_', ' ').title():12s}: {test_metrics[key]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Comparing Multiple Models\n",
    "\n",
    "Let's compare different model architectures to find the best approach for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_TO_COMPARE = [\n",
    "    CUSTOM_MODEL_NAME,                          # Custom lightweight transformer\n",
    "    f'{SIAMESE_PREFIX}{CUSTOM_MODEL_NAME}',     # Siamese custom transformer\n",
    "    'prajjwal1/bert-mini',                      # Pretrained BERT-mini\n",
    "    f'{SIAMESE_PREFIX}prajjwal1/bert-mini',     # Siamese BERT-mini\n",
    "    'microsoft/deberta-v3-small',               # (used by SCL-DeBERTa)\n",
    "    f'{SIAMESE_PREFIX}microsoft/deberta-v3-small',\n",
    "    'roberta-base',                             # (used by cornell-1 and better_call_claude)\n",
    "    f'{SIAMESE_PREFIX}roberta-base'\n",
    "]\n",
    "\n",
    "COMPARISON_FILE_PATH = 'model_comparison_results.csv'\n",
    "\n",
    "if os.path.exists(COMPARISON_FILE_PATH) and (not ALWAYS_TRAIN):\n",
    "    comparison_df = pd.read_csv(COMPARISON_FILE_PATH)\n",
    "    print(f\"\\nLoaded existing comparison from {COMPARISON_FILE_PATH}\")\n",
    "else:\n",
    "    comparison_df = compare_models(\n",
    "        model_names=MODELS_TO_COMPARE,\n",
    "        train_df=train_df,\n",
    "        val_df=validation_df,\n",
    "        test_df=test_df,\n",
    "        train_kwargs={'pretrained': {'freeze_encoder': 0.2}}\n",
    "    )\n",
    "\n",
    "    comparison_df.to_csv(COMPARISON_FILE_PATH, index=False)\n",
    "    print(f\"\\nComparison saved to {COMPARISON_FILE_PATH}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "successful = comparison_df[comparison_df['status'] == 'success']\n",
    "if len(successful) > 0:\n",
    "    print(\"\\nTest Set Results (sorted by F1):\")\n",
    "    display_cols = ['model', 'test_f1', 'test_accuracy', 'test_precision', 'test_recall', 'test_auc_roc']\n",
    "    print(successful[display_cols].sort_values('test_f1', ascending=False).to_string(index=False))\n",
    "    \n",
    "    best_model = successful.loc[successful['test_f1'].idxmax()]\n",
    "    print(f\"\\nBest model: {best_model['model']} (Test F1: {best_model['test_f1']:.4f})\")\n",
    "\n",
    "failed = comparison_df[comparison_df['status'] == 'failed']\n",
    "if len(failed) > 0:\n",
    "    print(f\"\\nFailed models: {', '.join(failed['model'].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Inference Examples\n",
    "\n",
    "## 6.1 Test on Dataset Samples\n",
    "\n",
    "Let's test the model on some examples from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer for inference\n",
    "model_name = CUSTOM_MODEL_NAME\n",
    "config = get_model_config(model_name, device)\n",
    "model = load_model_from_config(config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_path)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a few examples from the test set\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE PREDICTIONS FROM TEST SET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "num_examples = 5\n",
    "correct = 0\n",
    "\n",
    "for i in range(num_examples):\n",
    "    row = test_df.iloc[i]\n",
    "    test_sent1 = row['sentence1']\n",
    "    test_sent2 = row['sentence2']\n",
    "    true_label = row['label']\n",
    "    \n",
    "    pred, conf = predict_authorship_change(test_sent1, test_sent2, model, tokenizer, device, config.max_length)\n",
    "    \n",
    "    is_correct = pred == true_label\n",
    "    correct += int(is_correct)\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Sentence 1: {test_sent1[:80]}...\" if len(test_sent1) > 80 else f\"  Sentence 1: {test_sent1}\")\n",
    "    print(f\"  Sentence 2: {test_sent2[:80]}...\" if len(test_sent2) > 80 else f\"  Sentence 2: {test_sent2}\")\n",
    "    print(f\"  True label: {'Different authors' if true_label else 'Same author'}\")\n",
    "    print(f\"  Predicted:  {'Different authors' if pred else 'Same author'} (confidence: {conf:.2%})\")\n",
    "    print(f\"  Result:     {'Correct' if is_correct else 'Incorrect'}\")\n",
    "\n",
    "print(f\"\\nAccuracy on these examples: {correct}/{num_examples} ({correct/num_examples:.0%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Test on Custom Sentences\n",
    "\n",
    "You can try any model with by changing the sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Same author (similar formal writing style)\n",
    "sent1 = \"The implementation of machine learning algorithms requires careful consideration of various hyperparameters.\"\n",
    "sent2 = \"Furthermore, the selection of appropriate evaluation metrics is crucial for assessing model performance.\"\n",
    "\n",
    "pred, conf = predict_authorship_change(sent1, sent2, model, tokenizer, device, config.max_length)\n",
    "\n",
    "print(\"Example 1 - Expected: Same author (formal academic style)\")\n",
    "print(f\"  Sentence 1: {sent1}\")\n",
    "print(f\"  Sentence 2: {sent2}\")\n",
    "print(f\"  Prediction: {'Different authors' if pred else 'Same author'} (confidence: {conf:.2%})\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Different authors (contrasting styles)\n",
    "sent1 = \"The empirical analysis demonstrates a statistically significant correlation between the variables under investigation.\"\n",
    "sent2 = \"lol yeah that's pretty cool i guess, dunno why anyone would care tho tbh\"\n",
    "\n",
    "pred, conf = predict_authorship_change(sent1, sent2, model, tokenizer, device, config.max_length)\n",
    "\n",
    "print(\"Example 2 - Expected: Different authors (formal vs. casual)\")\n",
    "print(f\"  Sentence 1: {sent1}\")\n",
    "print(f\"  Sentence 2: {sent2}\")\n",
    "print(f\"  Prediction: {'Different authors' if pred else 'Same author'} (confidence: {conf:.2%})\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Comparison with Other Publications\n",
    "\n",
    "Now that we have developed and evaluated our models, we compare our best performing approach against state-of-the-art solutions from the PAN@CLEF 2025 Multi-Author Writing Style Analysis shared task. This comparison will help us understand how our best model performs relative to other approaches used by other research teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = \"TODO\"\n",
    "f1_easy = 0.000    # TODO\n",
    "f1_medium = 0.000  # TODO\n",
    "f1_hard = 0.000    # TODO\n",
    "\n",
    "sota_results = [\n",
    "    {\"Team\": \"better_call_claude\", \"Approach\": \"SSPC (BiLSTM/PLM)\", \"Easy\": 0.929, \"Medium\": 0.815, \"Hard\": 0.731},\n",
    "    {\"Team\": \"stylospies\", \"Approach\": \"Graph/Structural Features (SBERT/GB)\", \"Easy\": 0.959, \"Medium\": 0.786, \"Hard\": 0.791},\n",
    "    {\"Team\": \"TMU\", \"Approach\": \"Ensemble LaBSE/Siamese BiLSTM\", \"Easy\": 0.950, \"Medium\": 0.792, \"Hard\": 0.792},\n",
    "    {\"Team\": \"cornell-1\", \"Approach\": \"Ensembled-BertStyleNN\", \"Easy\": 0.909, \"Medium\": 0.793, \"Hard\": 0.698},\n",
    "    {\"Team\": \"OpenFact\", \"Approach\": \"Punctuation-Guided Pretraining\", \"Easy\": 0.919, \"Medium\": 0.771, \"Hard\": 0.752},\n",
    "    {\"Team\": \"xxsu-team\", \"Approach\": \"SCL-DeBERTa (SCL)\", \"Easy\": 0.955, \"Medium\": 0.825, \"Hard\": 0.829},\n",
    "    # Arturo + Radu: Arduo sounds like Arturo and contains all leters in Radu. It also means hard work, and since it ends with 'duo' it hints at collaboration.\n",
    "    {\"Team\": \"Arduo\", \"Approach\": f\"{best_model_name}\", \"Easy\": f1_easy, \"Medium\": f1_medium, \"Hard\": f1_hard}\n",
    "]\n",
    "\n",
    "sota_df = pd.DataFrame(sota_results)\n",
    "sota_df['Average F1'] = sota_df[['Easy', 'Medium', 'Hard']].mean(axis=1)\n",
    "\n",
    "sota_df_sorted = sota_df.sort_values('Average F1', ascending=False).round(3)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"COMPARISON WITH PUBLISHED PAN 2025 RESULTS\")\n",
    "print(\"Multi-Author Writing Style Analysis - F1 Macro Scores\")\n",
    "print(\"=\" * 100)\n",
    "display(sota_df_sorted.style.hide(axis='index').format(precision=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusion\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this project, we built and evaluated several transformer-based models for multi-author writing style analysis. Key findings:\n",
    "\n",
    "1. **Class Imbalance**: The dataset is heavily imbalanced. TODO: write more about this\n",
    "\n",
    "2. **Model Architectures**: We compared custom lightweight transformers with pretrained BERT models in both standard and siamese configurations.\n",
    "\n",
    "3. **Regularization**: Label smoothing and dropout helped reduce overfitting, which was a significant challenge.\n",
    "\n",
    "## Development Process\n",
    "\n",
    "### Complete Development History\n",
    "\n",
    "The full evolution of this project, including all experiments, failed attempts, and incremental improvements, is preserved in our [public GitHub repository commit history](https://github.com/RaduB20/multi_author_writing_style_analysis). Below, we explain the key decisions and their rationale so you don't need to dig through commits:\n",
    "\n",
    "### Development Timeline\n",
    "\n",
    "Our workflow and the implementations we did followed this progression:\n",
    "\n",
    "1. **Initial Data Pipeline**\n",
    "   - Basic `load_data()` function without augmentation\n",
    "   - Simple train/val/test splits\n",
    "\n",
    "2. **Pretrained Models Integration**\n",
    "   - HuggingFace transformer integration\n",
    "   - Training and evaluation pipeline for BERT-based models\n",
    "   - **Problem**: Training was slow (~X minutes per epoch) due to large parameter count\n",
    "   - **Solution**: Needed faster iteration cycles for debugging techniques and experimentation\n",
    "\n",
    "3. **Custom Architecture Development**\n",
    "   - Designed `LightweightTransformerEncoder` for rapid prototyping\n",
    "   - **Problem**: Code duplication between custom and pretrained pipelines\n",
    "   - **Solution**: Created `AuthorshipModel` class with HuggingFace-compatible interface for both types of models\n",
    "\n",
    "4. **Addressing Overfitting**\n",
    "   - **Critical observation**: Train accuracy ~90%, test accuracy ~50% (massive overfitting)\n",
    "   - Increased dropout from 0.1 -> 0.33\n",
    "   - Implemented label smoothing (0.1)\n",
    "   - Added gradient clipping (max_norm=1.0)\n",
    "   - Added weighted sampling for class imbalance\n",
    "   - Implemented data augmentation in `load_data()`\n",
    "\n",
    "5. **Siamese Architecture**\n",
    "   - **Motivation**: After establishing a solid baseline, we hypothesized that encoding sentences separately and comparing their representations might better capture stylistic differences\n",
    "   - Implemented multiple similarity methods: concatenation, difference, multiplication, cosine\n",
    "   - Flexible architecture allowing any combination of similarity features\n",
    "\n",
    "6. **Evaluation**\n",
    "   - `compare_models()` function for systematic model comparison\n",
    "   - Prediction interface for inference on new sentence pairs\n",
    "   - Training history visualization\n",
    "\n",
    "### On the Absence of Intermediate Versions\n",
    "\n",
    "This notebook contains only the working version because:\n",
    "- We never removed functionality, only added it\n",
    "- Each feature builds on the previous foundation\n",
    "- Showing intermediate versions would be redundant (they are subsets of the final code)\n",
    "\n",
    "## Comparison to State-of-the-Art\n",
    "\n",
    "TODO: Research and compare our approach to current SOTA methods for authorship verification and multi-author writing style analysis.\n",
    "\n",
    "## Future Improvements\n",
    "\n",
    "If we were to continue improving this project, some possible directions include:\n",
    "\n",
    "- **Apply Gridsearch**:\n",
    "- **Data Augmentation**: Using techniques like back-translation or paraphrasing to create more diverse training examples. Even SMOTE could be helpful when doing data augmentation from the vectorized sentences.\n",
    "- **Focal Loss**: A loss function specifically designed for imbalanced datasets that down-weights easy examples\n",
    "- **Larger Pretrained Models**: Fine-tuning larger models like DeBERTa or RoBERTa for better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n",
    "- PAN25 Multi-Author Analysis Task: [https://pan.webis.de/clef25/pan25-web/style-change-detection.html](https://pan.webis.de/clef25/pan25-web/style-change-detection.html)\n",
    "- Dataset: [https://zenodo.org/records/14891299](https://zenodo.org/records/14891299)\n",
    "- HuggingFace Transformers: [https://huggingface.co/transformers/](https://huggingface.co/transformers/)\n",
    "- MTEB Leaderboard: [https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n",
    "\n",
    "\n",
    "### Publicated Models References\n",
    "\n",
    "The external comparison results are based on the following publications from the PAN Lab at CLEF 2025 (provided in BibTeX format for easy citation):\n",
    "\n",
    "*   **Team stylospies**:\n",
    "    ```bibtex\n",
    "    @article{boriceanu2025style,\n",
    "      title={Style change detection using graph and structural-linguistic features for multi-author writing analysis},\n",
    "      author={Boriceanu, I and B{\\u{a}}ltoiu, A},\n",
    "      journal={Working Notes of CLEF},\n",
    "      year={2025}\n",
    "    }\n",
    "    ```\n",
    "*   **Team better\\_call\\_claude**:\n",
    "    ```bibtex\n",
    "    @article{schmidt2025team,\n",
    "      title={Team\" better\\_call\\_claude\": Style Change Detection using a Sequential Sentence Pair Classifier},\n",
    "      author={Schmidt, Gleb and R{\\\"o}misch, Johannes and Halchynska, Mariia and Gorovaia, Svetlana and Yamshchikov, Ivan P},\n",
    "      journal={arXiv preprint arXiv:2508.00675},\n",
    "      year={2025}\n",
    "    }\n",
    "    ```\n",
    "*   **Team OpenFact**:\n",
    "    ```bibtex\n",
    "    @article{ksikezniak2025openfact,\n",
    "      title={OpenFact at PAN 2025: punctuation-guided pretraining for sentence-level style change detection},\n",
    "      author={Ksi{\\k{e}}{\\.z}niak, Ewelina and W{\\k{e}}cel, Krzysztof and Sawi{\\'n}ski, Marcin},\n",
    "      journal={Working Notes of CLEF},\n",
    "      year={2025}\n",
    "    }\n",
    "    ```\n",
    "*   **Team xxsu-team**:\n",
    "    ```bibtex\n",
    "    @article{lin2025scl,\n",
    "      title={SCL-DeBERTa: multi-author writing style change detection enhanced by supervised contrastive learning},\n",
    "      author={Lin, Kaichuan and Liu, Chang and Ye, Fuchuan and Han, Zhongyuan},\n",
    "      journal={Working Notes of CLEF},\n",
    "      year={2025}\n",
    "    }\n",
    "    ```\n",
    "*   **Team TMU**:\n",
    "    ```bibtex\n",
    "    @article{hosseinbeigi2025team,\n",
    "      title={Team TMU at PAN 2025: an ensemble of fine-tuned LaBSE and siamese neural network for multi-author writing style analysis},\n",
    "      author={Hosseinbeigi, Sara Bourbour and Mehrani, Ali},\n",
    "      journal={Working Notes of CLEF},\n",
    "      year={2025}\n",
    "    }\n",
    "    ```\n",
    "*   **Team cornell-1**:\n",
    "    ```bibtex\n",
    "    @article{boloni2025team,\n",
    "      title={Team cornell-1 at PAN: ensembling fine-tuned transformer models for writing style analysis},\n",
    "      author={Boloni-Turgut, D and Verma, Dhriti and Cardie, Claire},\n",
    "      journal={Working Notes of CLEF},\n",
    "      year={2025}\n",
    "    }\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
